{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stall prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prediction of the pressure profil of an aircraft wing based on deep learning.\n",
    " Our goal is to use pressure predictions to prevent aircraft stalls.\n",
    " The advantage of deep learning in this case is that it requires very little computation compared with real-time simulations, and delivers rapid results that can be used by pilots or drones in their piloting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using data coming from the deepstall project : https://projects.asl.ethz.ch/datasets/doku.php?id=deepstall\n",
    "\n",
    "The data we have is in h5df, to visualize it, we can use : https://myhdf5.hdfgroup.org/\n",
    "\n",
    "Important question : Continuous regression or classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 dataset : one in wind tunnel and another in real flight condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Data Managing\n",
    "import seaborn as sns # visual tool\n",
    "import matplotlib.pyplot as plt # visual tool\n",
    "import numpy as np\n",
    "\n",
    "import h5py # to deal with h5p file\n",
    "\n",
    "\n",
    "\n",
    "file_FD = h5py.File('Flight_Data/FD_cal.h5', \"r\")\n",
    "file_WT = h5py.File('Wind_Tunnel/WT_cal.h5', \"r\")\n",
    "\n",
    "FD_keys = list(file_FD.keys())\n",
    "WT_keys = list(file_WT.keys())\n",
    "\n",
    "FD_attribute =list(file_FD[FD_keys[0]].keys())\n",
    "print(len(FD_attribute)) #548\n",
    "\n",
    "WT_attribute =list(file_WT[WT_keys[0]].keys())\n",
    "print(len(WT_attribute)) #506 \n",
    "\n",
    "data_WT = {}\n",
    "data_FD = {}\n",
    "list_keys_WT = list(file_WT.keys())\n",
    "list_keys_FD = list(file_FD.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for attr in FD_attribute:\n",
    "#    if attr not in WT_attribute:\n",
    "#        print(attr) # not null\n",
    "\n",
    "        \n",
    "#for attr in WT_attribute:\n",
    "#    if attr not in FD_attribute:\n",
    "#        print(attr) # not null\n",
    "        \n",
    "    \n",
    "# both data files are truncated to save only common fields\n",
    "common_attr = []\n",
    "for attr in WT_attribute:\n",
    "    if attr in FD_attribute:\n",
    "        common_attr.append(attr)\n",
    "to_remove = []        \n",
    "for attr in common_attr:\n",
    "    if 'rc_' in attr:\n",
    "        to_remove.append(attr)\n",
    "    if 'battery' in attr:\n",
    "        to_remove.append(attr)\n",
    "    if 'cpu' in attr:\n",
    "        to_remove.append(attr)\n",
    "    if 'actuator' in attr:\n",
    "        to_remove.append(attr)\n",
    "    if 'manual' in attr:\n",
    "        to_remove.append(attr)\n",
    "        \n",
    "for attr in to_remove:\n",
    "        common_attr.remove(attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned data column number : 370 uncleaned :548\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaned data column number : \" + str(len(common_keys)) + \" uncleaned :\"  + str(len(FD_attribute)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From h5d to dict and to pandas dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for each key of each h5d file we create a dict which will be later transform into a pandas dataframe\n",
    "dicts_list_WT = [{} for _ in range(len(list_keys_WT))]\n",
    "dicts_list_FD = [{} for _ in range(len(list_keys_FD))]\n",
    "\n",
    "# Populate the list of dict\n",
    "for _ in range(len(list_keys_WT)):\n",
    "    for attr in common_attr:\n",
    "        dicts_list_WT[_][attr] = file_WT[list_keys_WT[_]][attr][0]\n",
    "for _ in range(len(list_keys_FD)):\n",
    "    for attr in common_attr:\n",
    "        dicts_list_FD[_][attr] = file_FD[list_keys_FD[_]][attr][0]\n",
    "    \n",
    "global_dict_list = dicts_list_FD + dicts_list_WT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From dicts to pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an important decision is to be made here: are we going to mix all our data in one massive dataframe ? Or are we going to make small ones ?\n",
    "\n",
    "For each keys (for exemple '11_aoa_sweep_m5') of the dataframe there is an environmental context, for exemple the experiment as realized in a wind tunnel or outside, during a foggy day or not, using this attack angle or this one. All those informations have an impact on the data, so if we mix datas we also mix contexts and we lose the **contextual information**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we transform each dict into a pandas dataframe and check if there is not any constant field\n",
    "def rm_constant_field(df):\n",
    "    \"\"\"\n",
    "    Drops constant value columns of pandas dataframe.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    for column in df.columns:\n",
    "        if len(df[column].unique()) == 1:\n",
    "            result = result.drop(column,axis=1)\n",
    "    return result\n",
    "\n",
    "df_list = [pd.DataFrame() for _ in range(len(dicts_list_WT) + len(dicts_list_FD))]\n",
    "for _ in range(len(df_list)):\n",
    "    df_list[_] = pd.DataFrame.from_dict(global_dict_list[_])\n",
    "    #rm_constant_field(df_list[_]) might not be not a good idea\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
